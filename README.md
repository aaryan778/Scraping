# ğŸ’¼ Job Scraping System

**Automated job scraping system** that collects job listings from multiple sources (Google Jobs aggregator), extracts skills using AI/NLP, and provides an interactive dashboard for visualization and export.

## ğŸŒŸ Features

### âœ… **What This System Does**

- **ğŸ” Automated Scraping**: Scrapes jobs from Google Jobs (aggregates Indeed, LinkedIn, Glassdoor, ZipRecruiter, Monster, and 100+ job boards)
- **â° Hourly Updates**: Automatically refreshes job data every hour
- **ğŸ§  Skills Extraction**: Uses NLP to extract technical skills from job descriptions
- **ğŸ¯ Smart Classification**: Categorizes jobs into Frontend, Backend, Full Stack, Healthcare IT, etc.
- **ğŸŒ Multi-Country**: Supports US, Canada, India, and Australia
- **ğŸ“Š Interactive Dashboard**: Beautiful Streamlit dashboard with visualizations
- **ğŸ“¥ Excel Export**: Export jobs to Excel (IT only, Healthcare only, or combined)
- **ğŸš€ REST API**: FastAPI for programmatic access
- **ğŸ’¯ 100% Free**: No paid services required!

### ğŸ¯ **Target Industries & Roles**

**IT Roles:**
- Frontend: React, Vue.js, Angular, UI/UX developers
- Backend: Python, Java, Node.js, DevOps engineers
- Full Stack: Software engineers, full stack developers
- Specialized: Data engineers, ML engineers, Security engineers

**Healthcare IT:**
- EHR/EMR Developers
- FHIR/HL7 Developers
- Healthcare software engineers
- Medical informatics specialists
- Healthcare data analysts

### ğŸ“Š **Data Collected**

For each job:
- Title, Company, Location (Country, City)
- Job Description
- Skills (Required & Preferred) - 500+ skills database
- Category (Frontend/Backend/etc.)
- Industry (IT/Healthcare)
- Experience Level (Junior/Mid/Senior)
- Salary Range (when available)
- Source Platform (LinkedIn, Indeed, etc.)
- Remote/On-site status

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ **Installation**

```bash
# Clone the repository
git clone <repo-url>
cd Scraping

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Linux/Mac:
source venv/bin/activate
# On Windows:
# venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment file
cp .env.example .env
```

### 2ï¸âƒ£ **Setup**

```bash
# Initialize the database
python main.py --init-db
```

### 3ï¸âƒ£ **Run Everything (Easiest!)**

```bash
# This starts:
# - Scheduler (scrapes every hour)
# - API server (http://localhost:8000)
# - Dashboard (http://localhost:8501)

./run_all.sh
```

**That's it!** The system will:
1. Immediately start scraping jobs
2. Update every hour automatically
3. Provide dashboard at http://localhost:8501
4. Provide API at http://localhost:8000/docs

---

## ğŸ“– Usage Guide

### ğŸ¯ **Running Individual Components**

#### **Option 1: Run One-Time Scrape**
```bash
python main.py --scrape --max-jobs 50
```

#### **Option 2: Run Scheduler Only** (hourly updates)
```bash
python main.py --schedule --interval 1
```

#### **Option 3: Run Dashboard Only**
```bash
python main.py --dashboard
# Or directly:
streamlit run dashboard/app.py
```

#### **Option 4: Run API Only**
```bash
python main.py --api
# Or directly:
uvicorn api.main:app --reload
```

### ğŸ“Š **Using the Dashboard**

Open http://localhost:8501 in your browser.

**Features:**
- **Overview Tab**: Charts showing jobs by industry, category, experience level
- **Geographic Tab**: Jobs by country, city, remote vs on-site
- **Skills Tab**: Top skills in demand, skills by category
- **Job Listings Tab**: Browse and search all jobs
- **Export Tab**: Export to Excel (IT, Healthcare, or All)

**Filters:**
- Country (US, Canada, India, Australia)
- Industry (IT, Healthcare)
- Category (Frontend, Backend, Full Stack, etc.)
- Remote Only toggle

**Export Options:**
1. Export IT jobs only â†’ `IT_jobs.xlsx`
2. Export Healthcare jobs only â†’ `Healthcare_jobs.xlsx`
3. Export all jobs â†’ `All_jobs.xlsx`
4. Custom export with current filters

### ğŸ”Œ **Using the REST API**

API Documentation: http://localhost:8000/docs

**Example API Calls:**

```bash
# Get all jobs
curl http://localhost:8000/jobs

# Filter by country
curl http://localhost:8000/jobs?country=US

# Filter by industry
curl http://localhost:8000/jobs?industry=IT

# Filter by skill
curl http://localhost:8000/jobs?skill=python

# Get statistics
curl http://localhost:8000/stats

# Get top skills
curl http://localhost:8000/skills

# Get recent jobs (last 24 hours)
curl http://localhost:8000/recent-jobs?hours=24
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AUTOMATED SCHEDULER                       â”‚
â”‚                  (Runs every 1 hour)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   GOOGLE JOBS SCRAPER                        â”‚
â”‚  (Selenium - scrapes Indeed, LinkedIn, Glassdoor, etc.)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SKILLS EXTRACTION                           â”‚
â”‚    (NLP + Rule-based - extracts 500+ technical skills)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  JOB CLASSIFICATION                          â”‚
â”‚   (Categorizes: Frontend/Backend/Healthcare/etc.)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SQLITE DATABASE                            â”‚
â”‚            (Stores all job data)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                       â”‚
        â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REST API       â”‚                  â”‚   DASHBOARD      â”‚
â”‚  (FastAPI)       â”‚                  â”‚  (Streamlit)     â”‚
â”‚  Port 8000       â”‚                  â”‚  Port 8501       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
Scraping/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Environment variables template
â”œâ”€â”€ main.py                      # Main CLI entry point
â”œâ”€â”€ scheduler.py                 # Hourly scheduler
â”œâ”€â”€ run_all.sh                   # Run all services script
â”‚
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ job_categories.json      # Job categories (IT, Healthcare)
â”‚   â”œâ”€â”€ skills_database.json     # 500+ skills database
â”‚   â””â”€â”€ countries.json           # Countries configuration
â”‚
â”œâ”€â”€ models/                      # Database models
â”‚   â”œâ”€â”€ database.py              # SQLAlchemy models
â”‚   â”œâ”€â”€ schemas.py               # Pydantic schemas
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scrapers/                    # Scraping modules
â”‚   â”œâ”€â”€ google_jobs_scraper.py   # Google Jobs scraper (Selenium)
â”‚   â”œâ”€â”€ job_scraper_main.py      # Main orchestrator
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ processors/                  # Data processing
â”‚   â”œâ”€â”€ skills_extractor.py      # NLP skills extraction
â”‚   â”œâ”€â”€ job_classifier.py        # Job classification
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ api/                         # REST API
â”‚   â”œâ”€â”€ main.py                  # FastAPI application
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ dashboard/                   # Dashboard
â”‚   â””â”€â”€ app.py                   # Streamlit dashboard
â”‚
â”œâ”€â”€ data/                        # Data storage
â”‚   â”œâ”€â”€ raw/                     # Raw scraped data
â”‚   â”œâ”€â”€ processed/               # Processed data
â”‚   â””â”€â”€ exports/                 # Excel exports
â”‚
â”œâ”€â”€ logs/                        # Application logs
â””â”€â”€ tests/                       # Unit tests
```

---

## âš™ï¸ Configuration

### **Environment Variables** (`.env`)

```bash
# Database
DATABASE_URL=sqlite:///./jobs.db

# Scraping
SCRAPE_INTERVAL_HOURS=1
MAX_JOBS_PER_SEARCH=100

# Countries (comma-separated)
COUNTRIES=US,Canada,India,Australia

# Industries
INDUSTRIES=IT,Healthcare

# API
API_HOST=0.0.0.0
API_PORT=8000

# Dashboard
DASHBOARD_PORT=8501
```

### **Job Categories** (`config/job_categories.json`)

Customize which job titles to search for. Currently includes:
- **IT**: 50+ job titles (Frontend, Backend, Full Stack, Specialized)
- **Healthcare**: 20+ healthcare IT job titles

### **Skills Database** (`config/skills_database.json`)

500+ technical skills across:
- Programming languages
- Frontend frameworks
- Backend frameworks
- Databases
- Cloud platforms
- DevOps tools
- Healthcare-specific (HL7, FHIR, Epic, Cerner, etc.)

---

## ğŸ› ï¸ How It Works

### **1. Scraping Process**

1. **Search Google Jobs** for each job title + country combination
2. **Extract job cards** from search results
3. **Click each job** to get full details
4. **Parse job data**: title, company, location, description, URL, etc.
5. **Determine source**: LinkedIn, Indeed, Glassdoor, etc.

### **2. Skills Extraction**

Three methods combined:
1. **Direct matching**: Matches against 500+ skills database
2. **Pattern extraction**: "experience with X", "proficient in Y"
3. **Bullet point extraction**: Common in job descriptions

Separates into:
- **Required skills**: From "required", "must have" sections
- **Preferred skills**: From "nice to have", "preferred" sections

### **3. Classification**

**Industry**: IT or Healthcare (based on keywords)

**Category**:
- Frontend: React, Vue, Angular, etc.
- Backend: Python, Java, Node.js, etc.
- Full Stack: Software Engineer, Full Stack Developer
- Specialized: Data, ML, Security, QA
- Healthcare IT: EHR, EMR, FHIR developers
- Healthcare Data: Healthcare analysts

**Experience Level**: Junior, Mid, Senior (based on title and description)

### **4. Storage & Access**

- **Database**: SQLite (can upgrade to PostgreSQL)
- **API**: FastAPI provides REST endpoints
- **Dashboard**: Streamlit provides interactive UI
- **Export**: Pandas exports to Excel

---

## ğŸ”§ Customization

### **Add More Countries**

Edit `config/countries.json`:

```json
{
  "name": "Germany",
  "code": "DE",
  "google_jobs_location": "Germany",
  "major_cities": ["Berlin", "Munich", "Hamburg"]
}
```

### **Add More Job Titles**

Edit `config/job_categories.json`:

```json
"IT": {
  "Frontend": [
    "Your Custom Job Title"
  ]
}
```

### **Add More Skills**

Edit `config/skills_database.json`:

```json
"your_category": [
  "Skill 1", "Skill 2", "Skill 3"
]
```

### **Change Scraping Interval**

```bash
# Scrape every 2 hours
python scheduler.py --interval 2

# Scrape every 30 minutes
python scheduler.py --interval 0.5
```

### **Change Max Jobs Per Search**

```bash
python main.py --scrape --max-jobs 100
```

---

## ğŸ“Š Data Sources

The system scrapes **Google Jobs**, which aggregates from:

- âœ… LinkedIn
- âœ… Indeed
- âœ… Glassdoor
- âœ… ZipRecruiter
- âœ… Monster
- âœ… CareerBuilder
- âœ… SimplyHired
- âœ… Dice
- âœ… Company career pages
- âœ… 100+ other job boards

**Why Google Jobs?**
- Free, no API required
- Aggregates from all major job boards
- Always up-to-date
- No rate limiting issues
- Includes source URL to original posting

---

## ğŸ†˜ Troubleshooting

### **Issue: Chrome driver not found**
```bash
# The system auto-downloads ChromeDriver
# If issues persist, install manually:
pip install webdriver-manager --upgrade
```

### **Issue: Database locked**
```bash
# Stop all running services
pkill -f scheduler.py
pkill -f streamlit
pkill -f uvicorn

# Delete database and reinitialize
rm jobs.db
python main.py --init-db
```

### **Issue: No jobs scraped**
- Check internet connection
- Google may block automated requests temporarily (wait 15 min)
- Try running with `headless=False` to see what's happening
- Check logs in `logs/scheduler.log`

### **Issue: Skills not extracted**
- Skills extraction is rule-based, some jobs may have 0 skills
- Add more skills to `config/skills_database.json`
- Check job description format

---

## ğŸ¯ Performance

**Scraping Speed**:
- ~2-3 seconds per job
- ~50 jobs in 2-3 minutes
- 13 job titles Ã— 4 countries Ã— 50 jobs = ~2,600 jobs in ~2 hours

**Database**:
- SQLite handles 100K+ jobs easily
- Can upgrade to PostgreSQL for production

**Memory**:
- ~200-500 MB RAM during scraping
- ~100 MB for dashboard

---

## ğŸš€ Production Deployment

### **Option 1: Run on Server**

```bash
# Install as systemd service (Linux)
sudo nano /etc/systemd/system/job-scraper.service
```

```ini
[Unit]
Description=Job Scraper Service
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/path/to/Scraping
ExecStart=/path/to/venv/bin/python scheduler.py
Restart=always

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl enable job-scraper
sudo systemctl start job-scraper
```

### **Option 2: Docker** (Coming Soon)

### **Option 3: Cloud VM**
- Deploy on AWS EC2, Google Cloud VM, or DigitalOcean
- Run `./run_all.sh` in a tmux/screen session

---

## ğŸ“ˆ Future Enhancements

Potential improvements:
- [ ] Add more job sources (direct scraping)
- [ ] OpenAI integration for better skills extraction
- [ ] Email notifications for new jobs matching criteria
- [ ] Job recommendations based on skills
- [ ] Salary prediction ML model
- [ ] Mobile app
- [ ] Docker containerization
- [ ] Authentication for dashboard
- [ ] Job alerts via Telegram/Slack

---

## ğŸ“ License

MIT License - See LICENSE file

---

## ğŸ™ Credits

Built with:
- **Selenium**: Web scraping
- **FastAPI**: REST API
- **Streamlit**: Dashboard
- **SQLAlchemy**: Database ORM
- **Pandas**: Data processing
- **Plotly**: Visualizations
- **APScheduler**: Task scheduling
- **spaCy**: NLP (optional)

---

## ğŸ“§ Support

For issues, questions, or contributions:
- Open an issue on GitHub
- Check existing documentation
- Review logs in `logs/` directory

---

**Happy Job Hunting! ğŸš€**
